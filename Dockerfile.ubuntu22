# syntax=docker/dockerfile:1.4
ARG CUDA_VERSION_MINOR=12.4.1
ARG CUDA_VERSION_MAJOR=12.4
ARG BASE_IMAGE=nvidia/cuda:${CUDA_VERSION_MINOR}-cudnn-devel-ubuntu22.04
FROM ${BASE_IMAGE} AS base

ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get -qq update && \
        apt-get -qq install -y \
        --allow-change-held-packages \
        --no-install-recommends \
        --allow-downgrades \
        build-essential libtool autoconf automake autotools-dev unzip \
        ca-certificates \
        wget curl openssh-server vim environment-modules \
        iputils-ping net-tools \
        libnuma1 libsubunit0 libpci-dev \
        libpmix-dev \
        datacenter-gpu-manager \
        git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Mellanox OFED (latest)
RUN wget -qO - https://www.mellanox.com/downloads/ofed/RPM-GPG-KEY-Mellanox | apt-key add -
RUN cd /etc/apt/sources.list.d/ && wget https://linux.mellanox.com/public/repo/mlnx_ofed/latest/ubuntu22.04/mellanox_mlnx_ofed.list

RUN apt-get -qq update \
    && apt-get -qq install -y --no-install-recommends \
    ibverbs-utils libibverbs-dev libibumad3 libibumad-dev librdmacm-dev rdmacm-utils infiniband-diags ibverbs-utils \
    && rm -rf /var/lib/apt/lists/*
#         mlnx-ofed-hpc-user-only


FROM base AS builder-base
RUN apt-get -qq update && \
    apt-get -qq install -y --no-install-recommends \
      build-essential devscripts debhelper fakeroot pkg-config check && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*


FROM builder-base AS libnccl2
# NCCL
ARG TARGET_NCCL_VERSION='2.23.4-1'
ARG CUDA_ARCH_LIST='70 80 89 90'
# Converts CUDA_ARCH_LIST to '-gencode=arch=compute_XX,code=sm_XX -gencode=...' format with PTX for the last listed arch
RUN NVCC_GENCODE="$( \
    echo "${CUDA_ARCH_LIST}" | sed -e \
      's:\S\+:-gencode=arch=compute_\0,code=sm_\0:g; s:_[[:digit:]]\+$:\0 -gencode=arch=compute\0,code=compute\0:' \
    )" && \
    BUILD_THREADS="$(echo "${NVCC_GENCODE}" | wc -w)" && \
    mkdir /tmp/build && \
    cd /tmp/build && \
    wget -qO- "https://github.com/NVIDIA/nccl/archive/refs/tags/v${TARGET_NCCL_VERSION}.tar.gz" \
    | tar --strip-components=1 -xzf - && \
    NVCC_APPEND_FLAGS="--threads=${BUILD_THREADS}" \
      make -j20 pkg.debian.build NVCC_GENCODE="${NVCC_GENCODE}" && \
    cd build/pkg/deb && \
    ls -l && \
    mkdir /tmp/libnccl2 && \
    mv ./libnccl*.deb /tmp/libnccl2/ && \
    cd /tmp && \
    rm -r /tmp/build


FROM builder-base AS gdrcopy
# GDRCopy userspace components (2.4)
RUN mkdir /tmp/build /tmp/gdrcopy && \
    cd /tmp/build && \
    wget -qO- 'https://github.com/NVIDIA/gdrcopy/archive/refs/tags/v2.4.tar.gz' | tar xzf - && \
    CUDA=/usr/local/cuda ./gdrcopy-2.4/packages/build-deb-packages.sh -k && \
    mv ./gdrcopy-tests_2.4*.deb ./libgdrapi_2.4*.deb /tmp/gdrcopy/ && \
    cd /tmp && \
    rm -r /tmp/build


FROM builder-base AS perftest
# IB perftest with GDR
ENV PERFTEST_VERSION_HASH=5b47ede
RUN mkdir /tmp/build && \
    cd /tmp/build && \
    git clone https://github.com/coreweave/perftest && \
    cd perftest && \
    git checkout $PERFTEST_VERSION_HASH && \
    ./autogen.sh && \
    ./configure CUDA_H_PATH=/usr/local/cuda/include/cuda.h && \
    make -j20 && \
    mkdir /tmp/perftest && \
    make install DESTDIR=/tmp/perftest && \
    cd /tmp && \
    rm -r /tmp/build


FROM builder-base AS cuda-samples
# Build GPU Bandwidthtest from samples
ARG CUDA_SAMPLES_VERSION
RUN mkdir /tmp/build && \
    cd /tmp/build && \
    wget -qO- https://github.com/NVIDIA/cuda-samples/archive/refs/tags/v${CUDA_SAMPLES_VERSION}.tar.gz \
    | tar --strip-components=1 -xzf - && \
    cd Samples/1_Utilities/bandwidthTest && \
    make -j20 && \
    mkdir -p /tmp/cuda-samples/usr/bin && \
    install bandwidthTest /tmp/cuda-samples/usr/bin/ && \
    cd /tmp && \
    rm -r /tmp/build


FROM builder-base AS hpcx
# HPC-X
# grep + sed is used as a workaround to update hardcoded pkg-config / libtools archive / CMake prefixes
ARG HPCX_DISTRIBUTION="hpcx-v2.20-gcc-mlnx_ofed-ubuntu22.04-cuda12"
RUN cd /tmp && \
    DIST_NAME="${HPCX_DISTRIBUTION}-$(uname -m)" && \
    HPCX_DIR="/opt/hpcx" && \
    wget -q -O - "https://blobstore.object.ord1.coreweave.com/drivers/${DIST_NAME}.tbz" | tar xjf - && \
    grep -IrlF "/build-result/${DIST_NAME}" "${DIST_NAME}" | xargs -rd'\n' sed -i -e "s:/build-result/${DIST_NAME}:${HPCX_DIR}:g" && \
    mv "${DIST_NAME}" "${HPCX_DIR}" && \
    rm -r /opt/hpcx/ompi

# Rebuild OpenMPI to support SLURM
SHELL ["/bin/bash", "-c"]
RUN source /opt/hpcx/hpcx-init.sh && \
    hpcx_load && \
    cd /opt/hpcx/sources && \
    tar -zxvf openmpi-gitclone.tar.gz && \
    cd openmpi-gitclone && \
    ./configure -C --prefix=/opt/hpcx/ompi \
      --with-hcoll=/opt/hpcx/hcoll --with-ucx=/opt/hpcx/ucx \
      --with-platform=contrib/platform/mellanox/optimized \
      --with-slurm --with-hwloc --with-libevent \
      --with-pmix="/usr/lib/$(gcc -print-multiarch)/pmix2" \
      --without-xpmem --with-cuda --with-ucc=/opt/hpcx/ucc && \
      make -j20 && \
      make -j20 install && \
      cd .. && \
      rm -r openmpi-gitclone
SHELL ["/bin/sh", "-c"]


FROM base
RUN --mount=type=bind,from=libnccl2,source=/tmp/libnccl2,target=/tmp/install \
    cd /tmp/install && dpkg -i *.deb
RUN --mount=type=bind,from=gdrcopy,source=/tmp/gdrcopy,target=/tmp/install \
    cd /tmp/install && dpkg -i *.deb

COPY --link --from=perftest /tmp/perftest/ /
COPY --link --from=cuda-samples /tmp/cuda-samples/ /
COPY --link --from=hpcx /opt/hpcx /opt/hpcx

RUN ldconfig

# HPC-X Environment variables
COPY ./printpaths.sh /tmp
SHELL ["/bin/bash", "-c"]
RUN source /opt/hpcx/hpcx-init.sh && \
    hpcx_load && \
    # Uncomment to stop a run early with the ENV definitions for the below section
    # /tmp/printpaths.sh ENV && false && \
    # Preserve environment variables in new login shells \
    alias install='install --owner=0 --group=0' && \
    /tmp/printpaths.sh export \
      | install --mode=644 /dev/stdin /etc/profile.d/hpcx-env.sh && \
    # Preserve environment variables (except *PATH*) when sudoing
    install -d --mode=0755 /etc/sudoers.d && \
    /tmp/printpaths.sh \
      | sed -E -e '{ \
          # Convert NAME=value to just NAME \
          s:^([^=]+)=.*$:\1:g ; \
          # Filter out any variables with PATH in their names \
          /PATH/d ; \
          # Format them into /etc/sudoers env_keep directives \
          s:^.*$:Defaults env_keep += "\0":g \
        }' \
      | install --mode=440 /dev/stdin /etc/sudoers.d/hpcx-env && \
    # Register shared libraries with ld regardless of LD_LIBRARY_PATH
    echo $LD_LIBRARY_PATH | tr ':' '\n' \
      | install --mode=644 /dev/stdin /etc/ld.so.conf.d/hpcx.conf && \
    rm /tmp/printpaths.sh
SHELL ["/bin/sh", "-c"]

# The following envs are from the output of the printpaths ENV script.
# Uncomment "/tmp/printpaths.sh ENV" above to run the script
# as part of a Docker build. Copy-paste the updated output in here.
# These ENVs need to be updated on new HPC-X install, different base image
# or any path related modifications before this stage in the Dockerfile.

# Begin auto-generated paths
ENV HPCX_DIR=/opt/hpcx
ENV HPCX_UCX_DIR=/opt/hpcx/ucx
ENV HPCX_UCC_DIR=/opt/hpcx/ucc
ENV HPCX_SHARP_DIR=/opt/hpcx/sharp
ENV HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/opt/hpcx/nccl_rdma_sharp_plugin
ENV HPCX_HCOLL_DIR=/opt/hpcx/hcoll
ENV HPCX_MPI_DIR=/opt/hpcx/ompi
ENV HPCX_OSHMEM_DIR=/opt/hpcx/ompi
ENV HPCX_MPI_TESTS_DIR=/opt/hpcx/ompi/tests
ENV HPCX_OSU_DIR=/opt/hpcx/ompi/tests/osu-micro-benchmarks-7.2
ENV HPCX_OSU_CUDA_DIR=/opt/hpcx/ompi/tests/osu-micro-benchmarks-7.2-cuda
ENV HPCX_IPM_DIR=""
ENV HPCX_CLUSTERKIT_DIR=/opt/hpcx/clusterkit
ENV OMPI_HOME=/opt/hpcx/ompi
ENV MPI_HOME=/opt/hpcx/ompi
ENV OSHMEM_HOME=/opt/hpcx/ompi
ENV OPAL_PREFIX=/opt/hpcx/ompi
ENV OLD_PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
ENV PATH=/opt/hpcx/sharp/bin:/opt/hpcx/clusterkit/bin:/opt/hpcx/hcoll/bin:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/ompi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
ENV OLD_LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
ENV LD_LIBRARY_PATH=/opt/hpcx/nccl_rdma_sharp_plugin/lib:/opt/hpcx/ucc/lib/ucc:/opt/hpcx/ucc/lib:/opt/hpcx/ucx/lib/ucx:/opt/hpcx/ucx/lib:/opt/hpcx/sharp/lib:/opt/hpcx/hcoll/lib:/opt/hpcx/ompi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
ENV OLD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs
ENV LIBRARY_PATH=/opt/hpcx/nccl_rdma_sharp_plugin/lib:/opt/hpcx/ompi/lib:/opt/hpcx/sharp/lib:/opt/hpcx/ucc/lib:/opt/hpcx/ucx/lib:/opt/hpcx/hcoll/lib:/opt/hpcx/ompi/lib:/usr/local/cuda/lib64/stubs
ENV OLD_CPATH=""
ENV CPATH=/opt/hpcx/ompi/include:/opt/hpcx/ucc/include:/opt/hpcx/ucx/include:/opt/hpcx/sharp/include:/opt/hpcx/hcoll/include:
ENV PKG_CONFIG_PATH=/opt/hpcx/hcoll/lib/pkgconfig:/opt/hpcx/sharp/lib/pkgconfig:/opt/hpcx/ucx/lib/pkgconfig:/opt/hpcx/ompi/lib/pkgconfig:
# End of auto-generated paths

# Disable UCX VFS to stop errors about fuse mount failure
ENV UCX_VFS_ENABLE=no

# NCCL SHARP PLugin (master)
### Disabled as HPC-X has a recent enough version at this time
# RUN cd /tmp && \
#     wget -q https://github.com/Mellanox/nccl-rdma-sharp-plugins/archive/refs/heads/master.zip && \
#     unzip master.zip && \
#     cd nccl-rdma-sharp-plugins-master && \
#     ./autogen.sh && \
#     ./configure --with-cuda=/usr/local/cuda-${CUDA_VERSION_MAJOR} --prefix=/usr && \
#     make && \
#     make install && \
#     rm /opt/hpcx/nccl_rdma_sharp_plugin/lib/* && \
#     rm -r /tmp/*

# NCCL Tests
ENV NCCL_TESTS_COMMITISH=2cbb968
WORKDIR /opt/nccl-tests
RUN wget -q -O - https://github.com/NVIDIA/nccl-tests/archive/${NCCL_TESTS_COMMITISH}.tar.gz | tar --strip-components=1 -xzf - && \
    make -j20 MPI=1 && \
    ln -s /opt/nccl-tests /opt/nccl_tests

RUN ldconfig

# SSH dependencies for MPI
RUN sed -i 's/[ #]\(.*StrictHostKeyChecking \).*/ \1no/g' /etc/ssh/ssh_config && \
    echo "    UserKnownHostsFile /dev/null" >> /etc/ssh/ssh_config && \
    sed -i 's/#\(StrictModes \).*/\1no/g' /etc/ssh/sshd_config && \
    mkdir /var/run/sshd -p
